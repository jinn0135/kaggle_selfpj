{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-07T01:42:29.060561Z",
     "iopub.status.busy": "2023-03-07T01:42:29.060084Z",
     "iopub.status.idle": "2023-03-07T01:42:29.070091Z",
     "shell.execute_reply": "2023-03-07T01:42:29.068773Z",
     "shell.execute_reply.started": "2023-03-07T01:42:29.060518Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T01:42:29.109583Z",
     "iopub.status.busy": "2023-03-07T01:42:29.108649Z",
     "iopub.status.idle": "2023-03-07T01:42:29.135044Z",
     "shell.execute_reply": "2023-03-07T01:42:29.133876Z",
     "shell.execute_reply.started": "2023-03-07T01:42:29.109544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 12), (418, 11), (418, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./input/titanic/train.csv')\n",
    "test = pd.read_csv('./input/titanic/test.csv')\n",
    "submission = pd.read_csv('./input/titanic/gender_submission.csv')\n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.info())\n",
    "display(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class myDataTrans(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, age=True, farecut=True, cabinc=True, familysz=True, agecat=True, dummies=True):\n",
    "        self.age, self.farecut, self.cabinc = age, farecut, cabinc\n",
    "        self.familysz, self.agecat, self.dummies = familysz, agecat, dummies\n",
    "        self.dummy_cols, self.drop_cols = ['Sex','Embarked'], ['PassengerId','Ticket']\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.age: self.AgeName_li, self.meanAge_li = self.fill_Age(X)\n",
    "        if self.farecut: \n",
    "            self.fare_bin, self.fare_bin_label = self.mk_col_FareCut(X)\n",
    "            X['FareCut'] = pd.cut(X['Fare'], bins=self.fare_bin, right=False, labels=self.fare_bin_label)\n",
    "        if self.cabinc: self.Pclass_li, self.Cmode_li = self.fill_Cabin_C(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.age:\n",
    "            X['AgeName'] = X['Name'].str.extract('([A-Za-z]+)W.')\n",
    "            for i in range(len(self.AgeName_li)):\n",
    "                X.loc[(X['Age'].isnull())&(X['AgeName']==self.AgeName_li), 'Age'] = self.meanAge_li[i]\n",
    "            self.drop_cols.extend(['AgeName','Name'])\n",
    "        if self.farecut:\n",
    "            X['FareCut'] = pd.cut(X['Fare'], bins=self.fare_bin, right=False, labels=self.fare_bin_label)\n",
    "            X['FareCut'] = X['FareCut'].astype('float')\n",
    "            self.drop_cols.append('Fare')\n",
    "        if self.cabinc:\n",
    "            X['Cabin_C'] = X['Cabin'].str[:1]\n",
    "            for Pclass, mode_v in zip(self.Pclass_li, self.Cmode_li):\n",
    "                X.loc[(X['Cabin_C'].isnull())&(X['Pclass']==Pclass), 'Cabin_C'] = mode_v\n",
    "            self.dummy_cols.append('Cabin_C')\n",
    "            self.drop_cols.append('Cabin')\n",
    "        if self.familysz: \n",
    "            X['FamilySz'] = pd.cut(X['SibSp']+X['Parch']+1, bins=[0,1,2,4,12], \n",
    "                                   right=False, labels=['Single','SmallF','MedF','LargeF'])\n",
    "            self.dummy_cols.append('FamilySz')\n",
    "            self.drop_cols.extend(['SibSp','Parch'])\n",
    "        if self.agecat:\n",
    "            X['AgeCat'] = pd.cut(X['Age'], [-np.inf,18,25,35,60,np.inf], \n",
    "                                 right=False, labels=['Children', 'Youth', 'YoungAdult', 'MiddleAged', 'Senior'])\n",
    "            self.dummy_cols.append('AgeCat')\n",
    "            self.drop_cols.append('Age')\n",
    "        if self.dummies:\n",
    "            self.dummy_cols = list(set(self.dummy_cols))\n",
    "            X[self.dummy_cols] = X[self.dummy_cols].astype('str')\n",
    "        X.drop(columns=self.drop_cols, inplace=True)\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self, feature_names_in):\n",
    "        return feature_names_in\n",
    "        \n",
    "    def fill_Age(self, X): # Age\n",
    "        last_li = ['Capt','Col','Countess','Don','Dr','Jonkheer','Lady',\n",
    "                   'Dona','Major','Mlle','Mme','Ms','Rev','Sir']\n",
    "        replace_li = ['Mr','Other','Mrs','Mr','Mr','Other','Mrs','Miss',\n",
    "                      'Mr','Miss','Miss','Miss','Other','Mr']\n",
    "        X['AgeName'] = X['Name'].str.extract('([A-Za-z]+)W.')\n",
    "        fill_df = X.groupby('AgeName')['Age'].mean().to_frame().astype(int)\n",
    "        return list(fill_df.index), list(fill_df['Age'])    \n",
    "    \n",
    "    def mk_col_FareCut(self, X, n=10):\n",
    "        Fmax = int(X['Fare'].max())+1\n",
    "        Fmin = int(X['Fare'].min())\n",
    "        Fminmax = (Fmax-Fmin)//n - (Fmax-Fmin)%n\n",
    "        bins = list(range(Fmin,Fmax+Fminmax,Fminmax))\n",
    "        bins_label = [n+Fminmax/2 for n in bins[:-1]]\n",
    "        return bins, bins_label       \n",
    "    \n",
    "    def fill_Cabin_C(self, X):\n",
    "        X['Cabin_C'] = X['Cabin'].str[:1]\n",
    "        X.loc[(X['Pclass']==1)&((X['FareCut']==72.0)|(X['FareCut']==504.0)),'Cabin_C'] = 'B'\n",
    "        X.loc[(X['Pclass']==2)&(X['FareCut']==72.0),'Cabin_C'] = 'C'\n",
    "        fill_df = X.groupby('Pclass')['Cabin_C'].agg(**{'Cmode':lambda x:x.mode()})\n",
    "        return list(fill_df.index), list(fill_df['Cmode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "ct_impute = ColumnTransformer([\n",
    "    ('impute_num', SimpleImputer(strategy='median'), \n",
    "         make_column_selector(dtype_include=float)),\n",
    "    ('impute_cat', SimpleImputer(strategy='most_frequent'), \n",
    "         make_column_selector(dtype_include=object))], remainder='passthrough')\n",
    "ct_onehot = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(drop='if_binary', sparse_output=False), \n",
    "         make_column_selector(dtype_include=object))], remainder='passthrough')\n",
    "\n",
    "mypipe = Pipeline([('myTrans', myDataTrans(age=True, farecut=True, cabinc=True, \n",
    "                                        familysz=True, agecat=True, dummies=True)),\n",
    "                   ('simpleimpute', ct_impute),\n",
    "                   ('onehot', ct_onehot)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehot__impute_cat__Sex_male</th>\n",
       "      <th>onehot__impute_cat__Embarked_C</th>\n",
       "      <th>onehot__impute_cat__Embarked_Q</th>\n",
       "      <th>onehot__impute_cat__Embarked_S</th>\n",
       "      <th>onehot__impute_cat__Embarked_nan</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_A</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_B</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_C</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_D</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_E</th>\n",
       "      <th>...</th>\n",
       "      <th>onehot__impute_cat__FamilySz_MedF</th>\n",
       "      <th>onehot__impute_cat__FamilySz_SmallF</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Children</th>\n",
       "      <th>onehot__impute_cat__AgeCat_MiddleAged</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Senior</th>\n",
       "      <th>onehot__impute_cat__AgeCat_YoungAdult</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Youth</th>\n",
       "      <th>onehot__impute_cat__AgeCat_nan</th>\n",
       "      <th>remainder__impute_num__FareCut</th>\n",
       "      <th>remainder__remainder__Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   onehot__impute_cat__Sex_male  onehot__impute_cat__Embarked_C  \\\n",
       "0                           1.0                             0.0   \n",
       "1                           0.0                             1.0   \n",
       "2                           0.0                             0.0   \n",
       "\n",
       "   onehot__impute_cat__Embarked_Q  onehot__impute_cat__Embarked_S  \\\n",
       "0                             0.0                             1.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             1.0   \n",
       "\n",
       "   onehot__impute_cat__Embarked_nan  onehot__impute_cat__Cabin_C_A  \\\n",
       "0                               0.0                            0.0   \n",
       "1                               0.0                            0.0   \n",
       "2                               0.0                            0.0   \n",
       "\n",
       "   onehot__impute_cat__Cabin_C_B  onehot__impute_cat__Cabin_C_C  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            1.0   \n",
       "2                            0.0                            0.0   \n",
       "\n",
       "   onehot__impute_cat__Cabin_C_D  onehot__impute_cat__Cabin_C_E  ...  \\\n",
       "0                            0.0                            0.0  ...   \n",
       "1                            0.0                            0.0  ...   \n",
       "2                            0.0                            0.0  ...   \n",
       "\n",
       "   onehot__impute_cat__FamilySz_MedF  onehot__impute_cat__FamilySz_SmallF  \\\n",
       "0                                1.0                                  0.0   \n",
       "1                                1.0                                  0.0   \n",
       "2                                0.0                                  1.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Children  onehot__impute_cat__AgeCat_MiddleAged  \\\n",
       "0                                  0.0                                    0.0   \n",
       "1                                  0.0                                    1.0   \n",
       "2                                  0.0                                    0.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Senior  onehot__impute_cat__AgeCat_YoungAdult  \\\n",
       "0                                0.0                                    0.0   \n",
       "1                                0.0                                    0.0   \n",
       "2                                0.0                                    1.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Youth  onehot__impute_cat__AgeCat_nan  \\\n",
       "0                               1.0                             0.0   \n",
       "1                               0.0                             0.0   \n",
       "2                               0.0                             0.0   \n",
       "\n",
       "   remainder__impute_num__FareCut  remainder__remainder__Pclass  \n",
       "0                            24.0                             3  \n",
       "1                            72.0                             1  \n",
       "2                            24.0                             3  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehot__impute_cat__Sex_male</th>\n",
       "      <th>onehot__impute_cat__Embarked_C</th>\n",
       "      <th>onehot__impute_cat__Embarked_Q</th>\n",
       "      <th>onehot__impute_cat__Embarked_S</th>\n",
       "      <th>onehot__impute_cat__Embarked_nan</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_A</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_B</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_C</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_D</th>\n",
       "      <th>onehot__impute_cat__Cabin_C_E</th>\n",
       "      <th>...</th>\n",
       "      <th>onehot__impute_cat__FamilySz_MedF</th>\n",
       "      <th>onehot__impute_cat__FamilySz_SmallF</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Children</th>\n",
       "      <th>onehot__impute_cat__AgeCat_MiddleAged</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Senior</th>\n",
       "      <th>onehot__impute_cat__AgeCat_YoungAdult</th>\n",
       "      <th>onehot__impute_cat__AgeCat_Youth</th>\n",
       "      <th>onehot__impute_cat__AgeCat_nan</th>\n",
       "      <th>remainder__impute_num__FareCut</th>\n",
       "      <th>remainder__remainder__Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   onehot__impute_cat__Sex_male  onehot__impute_cat__Embarked_C  \\\n",
       "0                           1.0                             0.0   \n",
       "1                           0.0                             0.0   \n",
       "2                           1.0                             0.0   \n",
       "\n",
       "   onehot__impute_cat__Embarked_Q  onehot__impute_cat__Embarked_S  \\\n",
       "0                             1.0                             0.0   \n",
       "1                             0.0                             1.0   \n",
       "2                             1.0                             0.0   \n",
       "\n",
       "   onehot__impute_cat__Embarked_nan  onehot__impute_cat__Cabin_C_A  \\\n",
       "0                               0.0                            0.0   \n",
       "1                               0.0                            0.0   \n",
       "2                               0.0                            0.0   \n",
       "\n",
       "   onehot__impute_cat__Cabin_C_B  onehot__impute_cat__Cabin_C_C  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "\n",
       "   onehot__impute_cat__Cabin_C_D  onehot__impute_cat__Cabin_C_E  ...  \\\n",
       "0                            0.0                            0.0  ...   \n",
       "1                            0.0                            0.0  ...   \n",
       "2                            0.0                            0.0  ...   \n",
       "\n",
       "   onehot__impute_cat__FamilySz_MedF  onehot__impute_cat__FamilySz_SmallF  \\\n",
       "0                                0.0                                  1.0   \n",
       "1                                1.0                                  0.0   \n",
       "2                                0.0                                  1.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Children  onehot__impute_cat__AgeCat_MiddleAged  \\\n",
       "0                                  0.0                                    0.0   \n",
       "1                                  0.0                                    1.0   \n",
       "2                                  0.0                                    0.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Senior  onehot__impute_cat__AgeCat_YoungAdult  \\\n",
       "0                                0.0                                    1.0   \n",
       "1                                0.0                                    0.0   \n",
       "2                                1.0                                    0.0   \n",
       "\n",
       "   onehot__impute_cat__AgeCat_Youth  onehot__impute_cat__AgeCat_nan  \\\n",
       "0                               0.0                             0.0   \n",
       "1                               0.0                             0.0   \n",
       "2                               0.0                             0.0   \n",
       "\n",
       "   remainder__impute_num__FareCut  remainder__remainder__Pclass  \n",
       "0                            24.0                             3  \n",
       "1                            24.0                             3  \n",
       "2                            24.0                             2  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = mypipe.fit_transform(train.drop('Survived', axis=1))\n",
    "target = train['Survived'].copy()\n",
    "X_test = mypipe.transform(test.copy())\n",
    "display(X_train.head(3))\n",
    "display(X_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 24) (223, 24) (668,) (223,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df, train_target, valid_target = train_test_split(X_train, target)\n",
    "print(train_df.shape, valid_df.shape, train_target.shape, valid_target.shape)\n",
    "\n",
    "# tf.convert_to_tensor(train_df)\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train_df, train_target))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((valid_df, valid_target))\n",
    "# test_set = tf.data.Dataset.from_tensor_slices((X_test))\n",
    "\n",
    "batch_s = 16\n",
    "train_batches = train_set.shuffle(1000).batch(batch_s)\n",
    "valid_batches = valid_set.shuffle(1000).batch(batch_s)\n",
    "# test_batches = test_dataset.shuffle(1000).batch(batch_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val_accuracy: 35.75%\n"
     ]
    }
   ],
   "source": [
    "# simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training = model.fit(X_train, target, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "val_acc = np.mean(training.history['val_accuracy'])\n",
    "print(\"\\n%s: %.2f%%\" % ('val_accuracy', val_acc*100))\n",
    "# model.fit(train_batches, epochs=5, validation_data=valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden_layer1 = Dense(8, 24, activation='relu')\n",
    "        self.hidden_layer2 = Dense(8, 24, activation='relu')\n",
    "        self.output_layer = Dense(1, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = Dropout(0.2)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "def valid_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_11240\\375055457.py\", line 5, in train_step  *\n        loss = loss_object(labels, predictions)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(16,) and logits.shape=(1, 16)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11240\\4078976898.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalid_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_dsateew.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[1;34m(images, labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    140\u001b[0m                 )\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0min_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         )\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[0;32m   2076\u001b[0m       \u001b[0mSparse\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mcrossentropy\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2077\u001b[0m     \"\"\"\n\u001b[1;32m-> 2078\u001b[1;33m     return backend.sparse_categorical_crossentropy(\n\u001b[0m\u001b[0;32m   2079\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2080\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[0;32m   5658\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_symbolic_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5659\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5660\u001b[1;33m             res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\u001b[0m\u001b[0;32m   5661\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5662\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_11240\\375055457.py\", line 5, in train_step  *\n        loss = loss_object(labels, predictions)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Playdata\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(16,) and logits.shape=(1, 16)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_batches:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for valid_images, valid_labels in valid_batches:\n",
    "        valid_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Valid Loss: {valid_loss.result()}, '\n",
    "    f'Valid Accuracy: {valid_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 223)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_set\n",
    "len(train_set), len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 14)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch data\n",
    "batch_s = 16\n",
    "trainloader = DataLoader(train_set, batch_size=batch_s, shuffle=True)\n",
    "validloader = DataLoader(valid_set, batch_size=batch_s, shuffle=True)\n",
    "len(trainloader), len(validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "class FMnist_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # linear layer, fully connected layer, affine layer, dense layer : np.dot(x, w) + b\n",
    "        self.hidden_linear1 = nn.Linear(24,24) \n",
    "        self.batch_norm1 = nn.BatchNorm1d(24)\n",
    "        self.hidden_linear2 = nn.Linear(24,12)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(12)\n",
    "        self.hidden_linear3 = nn.Linear(12,6)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(6)\n",
    "        self.ouput_linear = nn.Linear(6,1)\n",
    "\n",
    "        # 가중치 초기화 (he초기화)\n",
    "        nn.init.kaiming_normal_(self.hidden_linear1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.hidden_linear2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.hidden_linear3.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.ouput_linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.3) # dropout\n",
    "        x = self.hidden_linear2(x)\n",
    "        x = self.batch_norm2(x) \n",
    "        x = F.relu(x)  \n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.hidden_linear3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.ouput_linear(x)    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, trainloader, validloader, optimizer, scheduler, epochs=17, patience=5):\n",
    "    n = 3; min_loss = 2000; trigger = 0\n",
    "    # print('-'*((epochs*n-15)//2),' now loading ','-'*((epochs*n-15)//2))\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # 훈련모드\n",
    "        train_loss = 0\n",
    "        for imgs, labels in trainloader:\n",
    "            imgs.resize_(imgs.shape[0], 784) # (100,1,28,28) -> (100,784)\n",
    "            loss = loss_fn(model(imgs), labels) \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # 검증\n",
    "        valid_loss, valid_accuracy = 0, 0\n",
    "        model.eval() # 평가 모드\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in validloader:\n",
    "                imgs.resize_(imgs.shape[0], 784)\n",
    "                logit = model(imgs)\n",
    "                _, preds = torch.max(logit, 1) # preds = logit.max(dim=1)[1]\n",
    "                valid_accuracy += int((preds==labels).sum())\n",
    "                valid_loss += loss_fn(logit, labels).item()\n",
    "\n",
    "        # print('-'*n, end='')\n",
    "        print('Epoch : {}/{}.......'.format(epoch+1, epochs),            \n",
    "                  'Train Loss : {:.3f}'.format(train_loss/len(trainloader)), \n",
    "                  'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)), \n",
    "                  'Valid Accuracy : {:.3f}'.format(valid_accuracy/(batch_s*len(validloader))))\n",
    "        writer.add_scalar('Train Loss', train_loss/len(trainloader), epoch+1)\n",
    "        writer.add_scalar('Valid Loss', valid_loss/len(validloader), epoch+1)\n",
    "        writer.add_scalars('TrainLoss and ValidLoss', {'Train':train_loss/len(trainloader),\n",
    "                                                      'Valid':valid_loss/len(validloader)}, epoch+1)\n",
    "        writer.add_scalar('Valid Accuracy', valid_accuracy/(batch_s*len(validloader)), epoch+1)\n",
    "\n",
    "        if valid_loss/len(validloader) > min_loss: # early stop\n",
    "            trigger += 1\n",
    "            if trigger > patience:\n",
    "                print('\\nEarly Stopping!! epoch/epochs: {}/{}'.format(epoch, epochs))\n",
    "                writer.flush()\n",
    "                break\n",
    "        else:\n",
    "            trigger = 0\n",
    "            min_loss = valid_loss/len(validloader) # min_loss 갱신\n",
    "            torch.save(model.state_dict(), 'best_checkpoint.pth') # valid loss가 커지기 전의 모델 저장\n",
    "\n",
    "        scheduler.step(valid_loss) # learning rate scheduler\n",
    "    writer.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_TensorSliceDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11240\\3705447825.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11240\\449768696.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, loss_fn, trainloader, validloader, optimizer, scheduler, epochs, patience)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 훈련모드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (100,1,28,28) -> (100,784)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_TensorSliceDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "lr = 0.001; epochs = 30; patience = 7\n",
    "model = FMnist_DNN()\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # optimizer\n",
    "# 모니터링하고 있는 값(예:valid_loss)의 최소값(min) 또는 최대값(max) patience 기간동안 줄어들지 않을 때(OnPlateau) lr에 factor(0.1)를 곱해주는 전략\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "train_loop(model, loss_fn, trainloader, validloader, optimizer, scheduler, epochs, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
